

Implemented using ES as primary search backend; DB fallback if ES is unavailable.

It is sufficient to store this table in a transactional database <span style="color:red;font-weight:bold;">-- ??? why not in no-sql database considering this will not be part of any transaction flow.</span>

<span style="color:red;font-weight:bold;">Future Scope (Hotel Entity)</span>

For search, push hotel data into Elasticsearch. replicate or index Hotel entity into Elasticsearch for efficient search, and maintain periodic backups or snapshots in object storage for recovery and auditing.

Images should go in object storage. 

```
Assuming each hotel has 10 images and each image is ~500 KB, the total storage required for storing all images would be: 1,000,000 × 10 × 500 KB = 5,000,000,000 KB = 5 TB.
```

<span style="color:red;font-weight:bold;">Future Scope (Inventory Entity)</span>

For **extremely high throughput, combine partitioning with horizontal sharding** (e.g., shard by `hotel_id` ranges or by region) so partitions are distributed across nodes.

---

Designing a scalable architecture begins with understanding the scale of the problem. **Back-of-the-envelope estimations** help us approximate critical metrics like **traffic load**, **storage requirements**, and **bandwidth usage**, which in turn influence key architectural decisions such as the number of **application servers**, **database sharding strategies**, **caching mechanisms**, and overall **network design**.

By doing these quick calculations early, we avoid over-engineering and ensure the system can gracefully handle both current and future demand. Let’s perform these estimations for our **Hotel Reservation System**.

![back-of-the-envelope-estimation](back-of-the-envelope-estimation.png)

---
### Bandwidth Estimation

**Bandwidth estimation** measures how much data flows into and out of the system each second. Quantifying this helps size **outbound network throughput**, justify **caching**, and decide where a **CDN** is essential.

<span style="color:red;">(in hotel-booking platform, egress throughput will be much higher than ingress throughput)</span>

Let's estimate the outbound network throughput for the core use cases:

<span style="color:purple;font-weight:bold">Search Hotels</span>

```
- Assumption: 20 Hotel Records sent per Request (pagination)
- Single Hotel Record Size ≈ 5 KB
- Payload Size = 20 * 5 KB = 100 KB

- Average RPS ≈ 250 RPS
- Average Throughput = 100 KB * 250 RPS 
					 = 25,000 KB/s
					 ≈ 25 MB/s
					 = 200 Mbps 

- Peak RPS ≈ 1,250 RPS
- Peak Throughput = 100 KB * 1,250 RPS
				  = 125,000 KB/s
				  ≈ 125 MB/s
				  = 1000 Mbps (~1 Gbps)
```

<span style="color:purple;font-weight:bold">View Details of Selected Hotel</span>

```
- Assumption: 10 images fetched sent per Request (load estimation)
- Payload Size = Single Hotel Record Size + 10 Images Size 
			   = 5 KB + 10 * 500 KB ≈ 5 MB 

- Average RPS ≈ 50 RPS
- Average Throughput = 5 MB * 50 RPS
					 = 250 MB/s
					 = 2000 Mbps (~2 Gbps)

- Peak RPS ≈ 250 RPS
- Peak Throughput = 5 MB * 250 RPS
				  = 1250 MB/s
				  = 10,000 Mbps (~10 Gbps)
```

<span style="color:green;font-weight:bold;">Summary</span>

| Operation          | Avg Throughput      | Peak Throughput        |
| ------------------ | ------------------- | ---------------------- |
| Search Hotels      | 200 Mbps            | 1000 Mbps (~1 Gbps)    |
| View Hotel Details | 2000 Mbps (~2 Gbps) | 10,000 Mbps (~10 Gbps) |

---
### System Architecture

Designing the system architecture begins with mapping the estimated load, storage, and bandwidth requirements into concrete design decisions.

The back-of-the-envelope estimations suggested **distinct traffic patterns** and **storage requirements** for each use case. A microservices architecture suits our hotel-booking platform because it allows us to **divide responsibilities into services that can be deployed and scaled independently**, ensuring each one is tailored to its specific workload.

The most important design decision in a microservices architecture is **where to place service boundaries**. For a hotel-reservation platform, the cleanest seams align with distinct domain responsibilities, data ownership, and traffic patterns:

---
### User Management using Clerk and Next.js

User management is a much bigger topic than simply allowing users to create accounts and sign in to your application.

Separate your public routes with from those that require a user to be signed in. <span style="color:green;font-weight:bold;background:beige;">In an application with dedicated backends and frontends, you'd typically separate the views in the frontend to prevent unauthorized users from accessing those views.</span>

---
### API Gateway

Protect the backend API routes so that tech savvy users can't bypass protections in the frontend.





---


<span style="color:purple;font-weight:bold;">User Service</span>

The **User Service** is the central source of truth for identity, exposing stable APIs for **login**, **signup**, **token refresh**, and **profile management**. 

Most of its traffic comes from **login** and **token refresh** <span style="color:red;">(do load estimation for token refresh)</span>, which are short-lived read / write operations that create moderate load but can spike during peak events (e.g., flash sales). <span style="color:green;font-weight:bold;background:beige;">A single instance can handle around 500 RPS, so 2–3 instances (across multiple AZs) behind a load balancer are sufficient for steady state, with autoscaling to 4–5 instances during surges.</span>

User data such as names, emails, phone numbers, password hashes, and roles is **transactional, relational, and security-sensitive**, making a relational database like `PostgreSQL` or `MySQL` the best fit, complemented by `Redis` for sessions, rate limiting, and short-lived profile caching <span style="color:red;">(understand this in depth)</span>. 

JWT validation can be offloaded to the `API Gateway` via the **User Service’s JWKS endpoint**, ensuring the service only handles identity lifecycle operations, making it lightweight and more CPU-bound (hashing, token signing) than bandwidth-heavy.

<span style="color:red;">How to develop user authentication and authorisation service for hotel reservation system?</span>

<span style="color:purple;font-weight:bold;">Hotel Service</span>

---

<span style="color:red;">(continue from here)</span>

why hotel service should not handle search hotel and book room requests?

hotels and rooms live in a sharded relational (or NewSQL) cluster to preserve transactional semantics while scaling horizontally: we partition by a hash of `hotel_id` for even load, or by region where data locality is important

a Details API for hotel metadata, Details at three pods (headroom for spikes),

Peak traffic is dominated by search (~1,250 RPS) and details reads; assuming ~500 RPS per stateless instance at P95 < 120 ms.

<span style="color:purple;font-weight:bold;">Search Service</span>

If the **Hotel Service** alone is responsible for serving user searches (e.g., “Find hotels in New York under $200 with Wi-Fi and breakfast”), every query would require the service to scan large amounts of hotel and room data directly from its relational database. 

As the number of hotels grows (millions globally), filtering on multiple attributes—price ranges, locations, amenities, availability—quickly becomes **expensive and slow**. Traditional relational queries may need complex joins across hotels and rooms, leading to **high query latency**, **heavy load on the primary database**, and degraded performance for critical operations like reservations that share the same database. In other words, the system risks becoming both **slow** and **unreliable** under search-heavy traffic.

The **Search Service** addresses this bottleneck by maintaining a **denormalized, inverted index** (e.g., in Elasticsearch or OpenSearch) optimized for fast text search and multi-field filtering. Instead of repeatedly hitting the transactional Hotel Service database, user search queries are directed to this index, which stores pre-flattened hotel documents containing names, locations, pricing snapshots, and amenity tags. Queries like “New York + Pool + under $200” can then be executed in **milliseconds** using inverted indexes, faceted search, and geospatial filtering—operations that relational databases are not optimized for.

Additionally, the Search Service can scale independently (more nodes for higher query throughput) and absorb thousands of read requests per second without affecting the **Hotel Service’s primary OLTP database**, which remains reserved for updates, writes, and consistency-critical operations. Updates to hotel records flow into the Search Service asynchronously via **change data capture (CDC)** or event streams, keeping the index fresh without slowing down the main transactional path.

**Search** (indexed, denormalized read model over catalog)

a Search API serving the high-read path, we start the Search API at four to five pods (with HPA on CPU/QPS),

Search is served from an <span style="color:red;font-weight:bold;">OpenSearch/Elasticsearch tier</span> with regional indices and hash shards; we denormalize the document (core hotel facets + a few room attributes) to keep search fast without fan-out reads.

<span style="color:purple;font-weight:bold;">Reservation Service</span>

a Booking service for ACID transactions, Booking at three pods (throughput is low but consistency and redundancy matter)

Bookings carry both a strong consistency requirement and natural time boundaries, so we use composite partitioning by month plus a secondary hash, retain 12 months online, and stream older partitions to an OLAP warehouse for analytics.

<span style="color:purple;font-weight:bold;">Inventory Service</span>

If the **Reservation Service** is used for every availability check, each search or detail request would need to scan existing reservations to determine whether rooms are still free for a given date range. At scale, with **millions of bookings**, this becomes very inefficient:

- **High read load on a write-heavy service:** the Reservation Service is optimized for ACID writes (creating/canceling bookings), not for read-heavy queries. Mixing the two creates lock contention, slowing down both searches and reservations.
    
- **Latency and scalability issues:** checking availability by querying raw bookings requires range scans and complex logic, leading to high response times under heavy search traffic (hundreds/thousands of RPS).
    
- **Coupling and reliability risks:** every search request hitting the Reservation Service directly ties user browsing to the transactional booking path. A spike in searches could overload reservations, causing booking failures and poor reliability.

A dedicated **Inventory Service** decouples **availability checks** from **reservation writes**, providing a scalable read-optimized path for search and detail queries.

- **Pre-aggregated inventory:** Instead of scanning bookings, the Inventory Service maintains **counters or calendars**(e.g., number of available rooms per hotel/room/date). Each booking increments or decrements these counters atomically.
    
- **Fast lookups:** Queries like “Is Room X available from Jan 10–12?” become simple counter or calendar lookups, which are extremely efficient compared to scanning all reservations.
    
- **Isolation from booking traffic:** The Reservation Service only writes booking data and publishes events (e.g., `BookingConfirmed`, `BookingCancelled`). The Inventory Service subscribes to these events and updates its availability state. This isolates high-volume reads from the write-heavy reservation workflow.
    
- **Scalability:** The Inventory Service can scale independently (e.g., horizontally on Redis or a purpose-built availability store) to handle thousands of RPS without affecting the transactional reservation path.
    
- **Consistency:** With short-TTL caching (5–30s) and event-driven updates, the Inventory Service provides near real-time accuracy while protecting the Reservation DB from being hammered by read traffic.

**Availability** (inventory and date-range checks)

an Availability service for fast date-range checks, Availability at three to four pods (low latency target),

<span style="color:purple;font-weight:bold;">Pricing Service</span>

If the Hotel Service also computes prices, every search/detail request forces it to join hotel+room data with **rate plans, seasons, taxes, currency, promotions, loyalty, and inventory**. That makes reads:

- **Slow & costly**: complex joins/recomputations per request under high RPS.
    
- **Coupled & brittle**: pricing rules/experiments collide with catalog changes; a pricing bug degrades all browsing.
    
- **Inconsistent**: surge/blackout updates race with reads; no clear versioning/audit trail.
    
- **Hard to scale**: you must scale the entire Hotel Service for what is really a pricing hotspot.  
    Result: higher latency, DB contention with bookings, and limited agility for promotions or dynamic pricing.
    

### How a dedicated Pricing Service fixes it

A **Pricing Service** owns all price logic and data, returning a **final, breakdowned price** for (hotel, room, date-range, market, user segment):

- **Right data model**: rate plans, calendars, seasons, taxes/fees, currency FX, coupons, B2B contracts—versioned and auditable.
    
- **Right computation strategy**:
    
    - **Precompute/cache** per (room-night, market, segment) with short TTLs;
        
    - **Compute-on-read** for edge cases;
        
    - **Event-driven invalidation** on rule/FX/inventory changes.
        
- **Right scaling/SLA**: scale Pricing horizontally without touching Catalog; keep a tight P95 (e.g., <80–120 ms) even during promos.
    
- **Consistency & transparency**: deterministic rules engine, **price versioning**, and a clear **price breakdown** (base, taxes, fees, discounts) for UI and audits.
    
- **Operational safety**: you can canary new promos/rules, roll back fast, and isolate failures (a pricing spike won’t take down hotel metadata).

**Pricing** (rules and dynamic rates). 

<span style="color:purple;font-weight:bold;">Additional Services</span>

<span style="color:red;">(Add small summary for additional services like user, payment, notification, logging & monitoring)</span> At the edge, a managed load balancer fronts an API gateway that handles routing, auth offload, and rate-limits. and a lightweight Auth service. Async workers handle notifications and payment orchestration so the synchronous path stays tight. Redis is deployed as a cluster so hot keys can be spread by consistent hashing, and object storage holds all media while the OLTP stores only URLs and metadata. Because images dominate bandwidth, a CDN is non-negotiable. We place a global CDN (CloudFront/Fastly/Akamai) in front of object storage and serve multiple responsive variants (thumb/medium/large) with automatic format negotiation (WebP/AVIF).


<span style="color:green;font-weight:bold;">AWS Architecture Diagram</span>

```
AWS System Architecture
```

---
### EXTRA

Everything is containerized on Kubernetes across at least two AZs so horizontal scaling absorbs bursts without single-AZ hotspots.

Operating in a distributed network means designing for failure, latency, and consistency. We deploy across multiple AZs by default and consider active-active read regions where regulatory or latency constraints apply.

bookings remain strongly consistent via a single logical writer per region or a consensus-backed database.

we coordinate cross-service workflows with the saga pattern and an outbox to ensure messages are never lost. Every network hop is wrapped with timeouts, retries with jitter, and circuit breakers to prevent retry storms. 
 
Rate-limits and bulkheads on the gateway keep non-critical traffic from starving critical paths. 

---

<span style="color:purple;font-weight:bold">Login User</span>

```

```


<span style="color:purple;font-weight:bold">Book a Hotel Room</span>

```
- Payload Size = 1 KB

- Average RPS = 1 RPS
- Average Throughput = 1 KB * 1 RPS = 1 KB/s ≈ 0.008 Mbps

- Peak RPS = 5 RPS
- Peak Throughput = 1 KB * 5 RPS = 5 KB/s ≈ 0.04 Mbps
```

---

<span style="color:purple;font-weight:bold;">User Service</span>

<span style="color:purple;font-weight:bold;">Payment Service</span>

<span style="color:purple;font-weight:bold;">Notification Service</span>

<span style="color:purple;font-weight:bold;">Logging & Notification Service</span>

---



<span style="color:green;font-weight:bold">Load Estimation Summary</span>

| Operation          | Total Requests / Day | Avg RPS | Peak RPS (5x) |
| ------------------ | -------------------- | ------- | ------------- |
| Login User         | 1,000,000            | 10      | 50            |
| Search Hotels      | 25,000,000           | 250     | 1,250         |
| View Hotel Details | 5,000,000            | 50      | 250           |
| Book Hotel Room    | 100,000              | 1       | 5             |

---

Use query parameters in the `GET /hotels` request because it aligns with REST best practices for read-only, idempotent operations like searching. Query parameters make the request easily cacheable, shareable, and bookmarkable, which improves performance and user experience. They are the standard way to send filters and sorting options in a `GET` request, unlike request bodies which are often ignored for GET and break caching. This approach works best for lightweight search criteria; however, if the payload becomes large or sensitive, using a `POST /search` endpoint with a request body is more appropriate.

```
GET /hotels?location=New+York&checkIn=2025-09-15&checkOut=2025-09-20&guests=2&rooms=1&priceMin=100&priceMax=500&sort=price_asc&amenities=wifi,pool
```

---


