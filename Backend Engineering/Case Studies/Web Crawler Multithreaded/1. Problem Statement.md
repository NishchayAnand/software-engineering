
A web crawler is a program that automatically traverses the web by downloading web pages and following links from one page to another. It is used to index the web for search engines, collect data for research, or monitor websites for changes.

Crawling (fetching HTML content) is typically **I/O-bound** and **slow**, often involving a network request to a remote server over the internet. If we do this **sequentially**, the program will waste time waiting for one page to return before checking the next.

<span style="color:green;font-weight:bold;background:beige;">By using multiple threads, we can fetch URLs from many pages simultaneously, drastically improving performance.</span>

---
### Problem Statement

Given a URL `startUrl` and an interface `HtmlParser`, implement **a Multi-threaded web crawler** to crawl all links that are under the **same hostname** as `startUrl`.

Return all URLs obtained by your web crawler in **any** order.

```
class Solution {
	public List<String> crawl(String startUrl, HtmlParser htmlParser) {}
}
```

The `HtmlParser` interface is defined as such:

```
interface HtmlParser {
	/*
	   Return a list of all urls from a webpage of given url. This is a blocking        call, that means it will do HTTP request and return when this request is         finished. However, it is guaranteed to return the URLs within 15ms.
	*/
  public List<String> getUrls(String url);
}
```

**Link:** [Leetcode 1242](https://leetcode.com/problems/web-crawler-multithreaded/?envType=problem-list-v2&envId=concurrency) | **Difficulty:** #Hard | **Asked in Companies:** #Anthropic `3`, #Google `2`, #MongoDB `2`, #DataBricks `5`, #OpenAI `4`, #Meta `2`, #Microsoft `2`, #Adobe `2`

---
### Functional Requirements

Your crawler should:

- Start from the page: `startUrl`
- Call `HtmlParser.getUrls(url)` to get all URLs from a webpage of a given URL.
- Do not crawl the same link twice.
- Explore only the links that are under the **same hostname** as `startUrl`.

![](https://assets.leetcode.com/uploads/2019/08/13/urlhostname.png)

> **NOTE:** For simplicity's sake, you may assume 

---
### Assumptions

- `startUrl` is one of the `urls`.
- All URLs use **HTTP protocol** without any **port** specified.
- Hostname label must be from `1` to `63` characters long, including the dots, may contain only the ASCII letters from `'a'` to `'z'`, digits from `'0'` to `'9'` and the hyphen-minus character (`'-'`).
- The hostname may not start or end with the hyphen-minus character ('-').
- You may assume there're **no duplicates** in the URL library.

---
### Sample Test Cases

For custom testing purposes, you'll have three variables `urls`, `edges` and `startUrl`. You will only have access to `startUrl` in your code, while `urls` and `edges` are not directly accessible to you in code.

**Example 1:**

```
Input:

urls = [
  "http://news.yahoo.com",                // index = 0
  "http://news.yahoo.com/news",           // index = 1
  "http://news.yahoo.com/news/topics/",   // index = 2
  "http://news.google.com",               // index = 3
  "http://news.yahoo.com/us"              // index = 4
]

edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]

startUrl = "http://news.yahoo.com/news/topics/"
```

![](https://assets.leetcode.com/uploads/2019/10/23/sample_2_1497.png)

```
Output: [
  "http://news.yahoo.com",
  "http://news.yahoo.com/news",
  "http://news.yahoo.com/news/topics/",
  "http://news.yahoo.com/us"
]
```

**Example 2:**

```
Input:

urls = [
  "http://news.yahoo.com",                // index = 0
  "http://news.yahoo.com/news",           // index = 1
  "http://news.yahoo.com/news/topics/",   // index = 2
  "http://news.google.com"                // index = 3
]

edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]

startUrl = "http://news.google.com"
```

![](https://assets.leetcode.com/uploads/2019/10/23/sample_3_1497.png)

```
Output: ["http://news.google.com"]
```

---
### Intuition

Think of each web page (URL) as a **node**, and each link on the page as a **directed edge** to another node. The problem resonates a classical **Graph Traversal** problem.



---
### Approach - DFS Graph Traversal

Starting from the `startUrl`, you want to **explore all reachable nodes** (pages) that:

- Belong to the **same domain (hostname)**.
- Have **not been visited** before.

---
### Follow up

1. Assume we have **10,000 nodes** and **1 billion URLs** to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimise communication between machines and make sure each node does equal amount of work. How would your web crawler design change?

2. What if one node fails or does not work?

3. How do you know when the crawler is done?

---

