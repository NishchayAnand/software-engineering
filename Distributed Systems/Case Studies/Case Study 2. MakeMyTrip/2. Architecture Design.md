
Designing a scalable architecture begins with understanding the scale of the problem.Â **Back-of-the-envelope estimations**Â help us approximate critical metrics like **traffic load**, **storage requirements**, and **bandwidth usage**, which in turn influence key architectural decisions such as the number ofÂ **application servers**,Â **database sharding strategies**,Â **caching mechanisms**, and overallÂ **network design**.

By doing these quick calculations early, we avoid over-engineering and ensure the system can gracefully handle both current and future demand. Letâ€™s perform these estimations for our **Hotel Reservation System**.

![back-of-the-envelope-estimation](back-of-the-envelope-estimation.png)

---
### Load Estimation

Understanding how much traffic the system will handle is essential for designing a robust and scalable architecture.Â 

The most critical step in this process is identifying theÂ **Daily Active Users (DAU)**. Once we know how many users are actively engaging with the platform each day, we can estimate <span style="color:green;font-weight:bold;background:beige;">peak hour load</span>, derive <span style="color:green;font-weight:bold;background:beige;">requests per second (RPS)</span>, and evaluate the <span style="color:green;font-weight:bold;background:beige;">read / write ratio</span> across different user actions.

```
- Assumption: Daily Active Users (DAU) = 5 Million
```

The next step is analyzingÂ <span style="color:green;font-weight:bold;background:beige;">how traffic distributes across different user actions</span>. Letâ€™s estimate the load for the three core use cases:Â 

<span style="color:purple;font-weight:bold">Search Hotels</span>

```
- Assumption: Average Search Requests per User per Day = 5

- Total Requests per Day = 5M * 5 = 25M
- Average RPS = 25,000,000 / (24 * 60 * 60) â‰ˆ 250 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 250 * 5 â‰ˆ 1,250 RPS
```

<span style="color:purple;font-weight:bold">View Details of Selected Hotel</span>

```
- Assumption: 50% of users view details after search
- Assumption: Average View Details Requests per User per Day = 2

- Total Requests per Day = 5M * 50% * 2 = 5,000,000
- Average RPS = 5,000,000 / (24 * 60 * 60) â‰ˆ 50 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 50 * 5 â‰ˆ 250 RPS
```

<span style="color:purple;font-weight:bold">Book a Hotel Room</span>

```
- Assumption: 2% of users make a booking per day
- Assumption: Average Booking Requests per User per Day = 1

- Total Requests per Day = 5M * 2% = 100,000
- Average RPS = 100,000 / (24 * 60 * 60) â‰ˆ 1 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 1 * 5 = 5 RPS
```

<span style="color:green;font-weight:bold">Summary</span>

| Operation          | Total Requests / Day | Avg RPS | Peak RPS (5x) |
| ------------------ | -------------------- | ------- | ------------- |
| Search Hotels      | 25,000,000           | 250     | 1,250         |
| View Hotel Details | 5,000,000            | 50      | 250           |
| Book Hotel Room    | 100,000              | 1       | 5             |

---
### Storage Estimation

Designing a scalable system requires a clear understanding of how much data the platform needs to store and manage over time.Â **Storage estimation**Â helps us anticipate database size, plan for indexing, implement backup strategies, and decide whether to adoptÂ **sharding or archiving**Â for future growth. 

Let's estimate the storage requirements for key entities:

<span style="color:purple;font-weight:bold">Hotel</span>

The **Hotel entity** serves as the foundation for the booking platform, providing the details users need to discover and evaluate hotels before making a reservation. It would contain metadata such as:

```
- Hotel ID: ~16 bytes (UUID or numeric ID)
- Name: ~100 bytes (typically under 50 characters)
- Location: ~250 bytes (full address, city, state, country)  
- Description: ~2 KB (maximum 2,000 characters)
- Amenities: ~500 bytes (list of 10 amenities, e.g., â€œWi-Fi, Poolâ€)
- Images Metadata: ~2 KB (10 image URLs, each ~200 bytes)
```

Based on these field-level estimates, we can now approximate the overall storage required for hotels:

```
- Assumption: Total Hotels = 1 Million
- Average Hotel Record Size = 5 KB (with indexes & overhead)
- Total Storage for Hotels = 1,000,000 Ã— 5 KB = 5 GB
```

> **NOTE:**Â AssumingÂ each hotel hasÂ **10 images**Â and each image is aroundÂ **500 KB**, the total storage required for storing all images would be: `1,000,000 Ã— 10 Ã— 500 KB = 5,000,000,000 KB = 5 TB`.

<span style="color:purple;font-weight:bold">Room</span>

If rooms were stored as anÂ **embedded list inside the Hotel entity**, updating a single room (e.g., changing its availability or price) would requireÂ **updating the entire hotel record**. This could lead toÂ **write conflicts**Â and requireÂ **locking the whole hotel record**, reducing performance under high load. 

By keepingÂ **Room**Â as a separate entity, each room can be updated independently, minimising lock contention and improvingÂ **scalability**,Â **performance**, andÂ **concurrency handling**.

```
- Room ID: ~16 bytes (UUID or numeric ID)
- Room Type: ~100 bytes (e.g., "Deluxe", "Standard")  
- Price: ~50 bytes (price value + currency)  
- Availability: ~250 bytes (calendar)  
- Other Attributes: ~500 bytes (bed type, occupancy, amenties, etc.)
```

```
- Assumption: Average Rooms per Hotel = 100
- Average Room Record Size = 1 KB (with indexes & overhead)  
- Total Storage for Rooms = 1,000,000 * 100 * 1 KB = 100 GB
```

<span style="color:purple;font-weight:bold">Users</span>

This entity forms the basis for identity management, personalization, and secure access across the platform.

```
- User ID:Â ~16 bytes (UUID or numeric ID)
- Name:Â ~100 bytes (First Name + Last Name)
- Email:Â ~100 bytes (can be long, e.g., corporate emails)
- Phone Number:Â ~20 bytes (include country code + separators)
- Hashed Password:Â ~256 bytes (depending on algorithm)
- Other Attributes:Â ~500 bytes (profile image URL, addresses, etc.)
```

```
- (Assumption) Total Registered Users: 50 Million
- Average User Record Size: 1 KB (with indexes & overhead)
- Total Storage = 50,000,000 * 1 KB = 50,000,000 KB = 50 GB
```

<span style="color:purple;font-weight:bold">Bookings</span>

This entityÂ represents the transactional data that links a user to a specific room in a hotel for a defined date range.

```
- User ID: ~16 bytesÂ (UUID or numeric ID)
- Hotel ID: ~16 bytes
- Room ID:Â ~16 bytes
- Dates: ~32 bytes (check-in and check-out dates)
- Payment status:Â ~20 bytes (e.g., â€œPaidâ€, â€œPendingâ€)
- Other Attributes:Â ~500 bytes (created timestamp,Â updated timestamp, etc.)
```

SinceÂ booking recordsÂ are transactional in nature, they are most frequently queried within the firstÂ **12 months**Â (for cancellations, modifications, customer support, disputes, etc.). After this retention window, the likelihood of accessing historical bookings drops significantly.

```
- Assumption: Daily Bookings = 100,000 (from load estimation)
- Assumption: Booking Retention = 1 year (365 days)
- Total Bookings per Year = 100,000 * 365 = 36.5 Million
- Booking Record Size = 1 KB (with indexes & overhead)
- Total Storage for Bookings = 36,500,000 * 1 KB = 36.5 GB
```

> **NOTE:** To optimize performance and reduce storage costs, move older bookingsÂ from theÂ **OLTP (Online Transaction Processing) database**Â to anÂ **OLAP (Online Analytical Processing) database**.

<span style="color:green;font-weight:bold">Summary</span>

| **Entity**       | **Estimated Size per Record** | **Total Records** | **Total Storage** |
| ---------------- | ----------------------------- | ----------------- | ----------------- |
| **Hotels**       | 5 KB                          | 1,000,000         | 5 GB              |
| **Hotel Images** | 500 KB                        | 10,000,000        | 5 TB              |
| **Rooms**        | 1 KB                          | 100,000,000       | 100 GB            |
| **Users**        | 1 KB                          | 50,000,000        | 50 GB             |
| **Bookings**     | 1 KB                          | 36,500,000        | 36.5 GB           |

---
### Bandwidth Estimation

**Bandwidth estimation**Â measures how much data flows into and out of the system each second. Quantifying this helps sizeÂ **network throughput**, justifyÂ **caching**, and decide where aÂ **CDN**Â is essential.

Let's estimate the network throughput for the three core use cases:

<span style="color:purple;font-weight:bold">Search Hotels</span>

```
- Assumption: 20 Hotel Records sent per Request (pagination)
- Single Hotel Record Size â‰ˆ 5 KB
- Payload Size = 20 * 5 KB = 100 KB

- Average RPS â‰ˆ 250 RPS
- Average Throughput = 100 KB * 250 RPS 
					 = 25,000 KB/s
					 â‰ˆ 25 MB/s
					 = 200 Mbps 

- Peak RPS â‰ˆ 1,250 RPS
- Peak Throughput = 100 KB * 1,250 RPS
				  = 125,000 KB/s
				  â‰ˆ 125 MB/s
				  = 1000 Mbps (~1 Gbps)
```

<span style="color:purple;font-weight:bold">View Details of Selected Hotel</span>

```
- Assumption: 10 images fetched sent per Request (load estimation)
- Payload Size = Single Hotel Record Size + 10 Images Size 
			   = 5 KB + 10 * 500 KB â‰ˆ 5 MB 

- Average RPS â‰ˆ 50 RPS
- Average Throughput = 5 MB * 50 RPS
					 = 250 MB/s
					 = 2000 Mbps (~2 Gbps)

- Peak RPS â‰ˆ 250 RPS
- Peak Throughput = 5 MB * 250 RPS
				  = 1250 MB/s
				  = 10,000 Mbps (~10 Gbps)
```

<span style="color:purple;font-weight:bold">Book a Hotel Room</span>

```
- Payload Size = 1 KB

- Average RPS = 1 RPS
- Average Throughput = 1 KB * 1 RPS = 1 KB/s â‰ˆ 0.008 Mbps

- Peak RPS = 5 RPS
- Peak Throughput = 1 KB * 5 RPS = 5 KB/s â‰ˆ 0.04 Mbps
```

<span style="color:green;font-weight:bold;">Summary</span>

| Operation          | Avg Throughput      | Peak Throughput        |
| ------------------ | ------------------- | ---------------------- |
| Search Hotels      | 200 Mbps            | 1000 Mbps (~1 Gbps)    |
| View Hotel Details | 2000 Mbps (~2 Gbps) | 10,000 Mbps (~10 Gbps) |
| Book Room          | 0.008 Mbps          | 0.04 Mbps              |

---
### System Architecture

Designing the system architecture begins with mapping the estimated load, storage, and bandwidth requirements into concrete design decisions.

The back-of-the-envelope estimations suggested **distinct traffic patterns** and **storage requirements** for each use case. A microservices architecture suits our hotel-booking platform because it allows us toÂ **divide responsibilities into services that can be deployed and scaled independently**, ensuring each one is tailored to its specific workload.

The most important design decision in a microservices architecture isÂ **where to place service boundaries**. For a hotel-reservation platform, the cleanest seams align with distinct domain responsibilities, data ownership, and traffic patterns:

<span style="color:purple;font-weight:bold;">User Service</span>

It manages user-related information such as name, email, phone number, password hashes, roles and permissions. Because this data isÂ **transactional, relational, and security-critical**, aÂ **relational database**Â such as `PostgreSQL` or `MySQL` is best suited.

The service would primarily be responsible for handling ...

---

to support high-volume reads, the database can be deployed with aÂ <span style="color:green;font-weight:bold;background:beige;">primary + read replicas</span> setup. The primary node handles writes (e.g., signups, password changes), while replicas serve read-heavy operations (e.g., profile lookups). 

From aÂ **traffic perspective**, authentication and user profile operations generateÂ **moderate load** -> a few hundred requests per second, can spike into the low thousands of requests per second during traffic surges (e.g., flash sales, peak travel seasons). A lightweight service like this can handleÂ **~500 RPS per instance**Â comfortably, since most calls are short-lived database reads/writes. To cover both average and peak loads:

- **Average:**Â 2â€“3 instances (multi-AZ) are sufficient for steady-state traffic.
- **Peak:**Â With autoscaling enabled, the service can expand toÂ **4â€“5 instances**Â to sustain bursts while keeping latency low (<100 ms at P95). Redis is used as aÂ **complementary cache**Â for sessions, rate-limiting, and short-lived profile lookups, which further reduces pressure on the primary database.

```
**Capacity per instance:**Â A lightweight stateless User Service instance (JWT validation + DB lookup) can handleÂ **~300â€“400 RPS**.
```

> **NOTE:** Sensitive personally identifiable information (PII) is encrypted at rest, and access is restricted to the User Service only.

- **Session & tokens:**Â UseÂ **Redis**Â for short-lived data like session states, refresh tokens, and rate-limits.
    
- **User profile cache:**Â Cache basic user profile data in Redis (`user:{id}`) with TTL of 5â€“10 minutes to reduce DB lookups for common requests.

start withÂ **3 instances**Â (multi-AZ) and autoscale toÂ **4â€“6**Â at peak; keep user data in aÂ **small, indexed relational store**, sessions/tokens inÂ **Redis**, and logs inÂ **cheap object storage**. This gives you low latency, strong security, and resilience without overprovisioning.

**nstances to handle average & peak.**Â JWTÂ **validation is offline**Â (done at gateway/services via public keys), so the User Service mainly handlesÂ **login, signup, refresh, profile CRUD**.

---

Stores PII securely in a relational store, uses Redis for sessions/rate limits, and exposes stable auth/identity APIs to other services. Priorities: security, low latency for auth flows, high availability.

**Ephemeral state (Redis):**Â sessions, refresh-token allow-list, rate limits.

Throughput per stateless instance (with bcrypt/Argon2, DB/Redis latency, P95 â‰¤ 100 ms):Â **~150 RPS**Â is a conservative target.

<span style="color:purple;font-weight:bold;">Hotel Service</span>

Owns identity, authentication, and user profiles. Handles signup/login, token issuance/refresh, preferences, and account management. Stores PII securely in a relational store, uses Redis for sessions/rate limits, and exposes stable auth/identity APIs to other services. Priorities: security, low latency for auth flows, high availability.

<span style="color:purple;font-weight:bold;">Search Service</span>



<span style="color:purple;font-weight:bold;">Reservation Service</span>

Owns identity, authentication, and user profiles. Handles signup/login, token issuance/refresh, preferences, and account management. Stores PII securely in a relational store, uses Redis for sessions/rate limits, and exposes stable auth/identity APIs to other services. Priorities: security, low latency for auth flows, high availability.

<span style="color:purple;font-weight:bold;">Inventory Service</span>

<span style="color:purple;font-weight:bold;">Pricing Service</span>

<span style="color:purple;font-weight:bold;">Notification Service</span>

Owns user communications (email/SMS/push). Consumes domain events (e.g., booking confirmations/changes) asynchronously from a queue/stream, applies templates/localization, and delivers messages via third-party providers with retries and DLQs. Keeps its own delivery logs/receipts for audit and replays. Priorities: decoupling, delivery reliability, and observability.

<span style="color:green;font-weight:bold;">AWS Architecture Diagram</span>

```
AWS System Architecture
```

---

Considering when the system scale, we may need to perform personalized search, Inventory Service, Pricing Service - real-time pricing

**Search**Â (indexed, denormalized read model over catalog),Â **Availability**Â (inventory and date-range checks),Â ,Â ,Â ,Â **Media**Â (image ingest/variants to object storage + CDN),Â **Pricing**Â (rules and dynamic rates), andÂ . Each of these has distinct traffic patterns, consistency needs, and change cadenceâ€”perfect signals for a separate boundary.

what services we need, number of service instance to handle all load and throughput.

How to distribute the data.

Where to add caching 

how to incorporate CDN

how to handle distributed network

---

We size the service layer directly from the load and bandwidth estimates. At the edge, a managed load balancer fronts an API gateway that handles routing, auth offload, and rate-limits. Behind it, we keep services small and independently scalable: a Search API serving the high-read path, a Details API for hotel metadata, an Availability service for fast date-range checks, a Booking service for ACID transactions, and a lightweight Auth service. Peak traffic is dominated by search (~1,250 RPS) and details reads; assuming ~500 RPS per stateless instance at P95 < 120 ms, we start the Search API at four to five pods (with HPA on CPU/QPS), Details at three pods (headroom for spikes), Availability at three to four pods (low latency target), and Booking at three pods (throughput is low but consistency and redundancy matter). Async workers handle notifications and payment orchestration so the synchronous path stays tight. Everything is containerized on Kubernetes across at least two AZs so horizontal scaling absorbs bursts without single-AZ hotspots.

Data distribution follows access patterns and growth. For OLTP, hotels and rooms live in a sharded relational (or NewSQL) cluster to preserve transactional semantics while scaling horizontally: we partition by a hash ofÂ `hotel_id`Â for even load, or by region where data locality is important. Bookings carry both a strong consistency requirement and natural time boundaries, so we use composite partitioning by month plus a secondary hash, retain 12 months online, and stream older partitions to an OLAP warehouse for analytics. Users sit in a single logical cluster with read replicas because read pressure is modest. Search is served from an OpenSearch/Elasticsearch tier with regional indices and hash shards; we denormalize the document (core hotel facets + a few room attributes) to keep search fast without fan-out reads. Redis is deployed as a cluster so hot keys can be spread by consistent hashing, and object storage holds all media while the OLTP stores only URLs and metadata.

Caching is layered to attack the biggest byte and query hotspots first. At the application tier we cache the first pages of popular searches and hotel details in Redis with short TTLs (minutes) and versioned keys so writes can invalidate only affected entries. Availability is cached aggressively with very short TTLs (seconds) and write-through invalidation on successful bookings; even modest hit rates here shave milliseconds off end-to-end latency and protect the primary store. On the client and edge we leverage HTTP caching semanticsâ€”immutable, fingerprinted JS/CSS and pre-generated thumbnailsâ€”so repeat visits incur minimal transfer. With target hit rates of 60â€“80% on Search and Details, we reduce both database reads and app-layer Mbps, which is the cheapest performance gain you can buy.

Because images dominate bandwidth, a CDN is non-negotiable. We place a global CDN (CloudFront/Fastly/Akamai) in front of object storage and serve multiple responsive variants (thumb/medium/large) with automatic format negotiation (WebP/AVIF). Cache keys include important vary dimensions (width/device), and invalidation is event-driven when a hotel updates photos. With an 80% edge hit ratio, origin egress for details drops from multi-Gbps peaks to a manageable fraction, while the CDN absorbs 2+ Gbps at the edge with lower latency for users. We keep signed URLs for partner uploads and enforce least-privilege object policies so media access doesnâ€™t widen the blast radius.

Operating in a distributed network means designing for failure, latency, and consistency. We deploy across multiple AZs by default and consider active-active read regions where regulatory or latency constraints apply; bookings remain strongly consistent via a single logical writer per region or a consensus-backed database, and we coordinate cross-service workflows with the saga pattern and an outbox to ensure messages are never lost. Every network hop is wrapped with timeouts, retries with jitter, and circuit breakers to prevent retry storms. Rate-limits and bulkheads on the gateway keep non-critical traffic from starving critical paths. Global DNS with health checks provides fast failover, while distributed tracing (W3C trace context), RED/USE dashboards, and SLOs (e.g., Search P99 < 300 ms, Booking success â‰¥ 99.9%) make regressions obvious. Security is zero-trust by defaultâ€”mTLS between services, a WAF at the edge, secret management for credentials, and signed CDN URLs for private content.

In short, the services are sized from your peak RPS and Mbps, the data is placed where it scales and stays consistent, cache sits exactly where it cuts the most queries and bytes, the CDN carries the image load, and the network is engineered to degrade gracefully under real-world failure and burst conditions.

---


---

ðŸ‘‰ Would you like me to also give aÂ **numerical example**Â (e.g., assume 10M DAU â†’ how many login/token refresh requests per second, how many replicas/instances needed)?