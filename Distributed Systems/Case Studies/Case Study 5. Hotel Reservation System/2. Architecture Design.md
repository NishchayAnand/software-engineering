
Designing a scalable architecture begins with understanding the scale of the problem. **Back-of-the-envelope estimations** help us approximate critical metrics like **traffic load**, **storage requirements**, and **bandwidth usage**, which in turn influence key architectural decisions such as the number of **application servers**, **database sharding strategies**, **caching mechanisms**, and overall **network design**.

By doing these quick calculations early, we avoid over-engineering and ensure the system can gracefully handle both current and future demand. Let’s perform these estimations for our **Hotel Reservation System**.

![back-of-the-envelope-estimation](back-of-the-envelope-estimation.png)

---
### Load Estimation

Understanding **how much traffic the system will handle** is crucial for designing a scalable and resilient architecture. It allows us to size infrastructure appropriately, anticipate bottlenecks, and decide where optimizations like **caching** or **load balancing** are required.

To quantify this, let’s estimate the <span style="color:green;font-weight:bold;background:beige;">Requests Per Second (RPS)</span> the system must support under both normal and peak load conditions for core user interactions:

<span style="color:purple;font-weight:bold">Login User</span>

```
- Assumption: Daily Active Users (DAU) = 5 Million
- Assumption: 20% of DAU log in per day

- Total Requests per Day = 5M * 20% = 1M
- Average RPS = 1,000,000 / (24 * 60 * 60) ≈ 10 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 10 * 5 ≈ 50 RPS
```

<span style="color:purple;font-weight:bold">Search Hotels</span>

```
- Assumption: Average Search Requests per User per Day = 5

- Total Requests per Day = 5M * 5 = 25M
- Average RPS = 25,000,000 / (86,400) ≈ 250 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 250 * 5 ≈ 1,250 RPS
```

<span style="color:purple;font-weight:bold">View Details of Selected Hotel</span>

```
- Assumption: 50% of users view details after search
- Assumption: Average View Details Requests per User per Day = 2

- Total Requests per Day = 5M * 50% * 2 = 5,000,000
- Average RPS = 5,000,000 / (86,400) ≈ 50 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 50 * 5 ≈ 250 RPS
```

<span style="color:purple;font-weight:bold">Book a Hotel Room</span>

```
- Assumption: 2% of users make a booking per day
- Assumption: Average Booking Requests per User per Day = 1

- Total Requests per Day = 5M * 2% = 100,000
- Average RPS = 100,000 / (24 * 60 * 60) ≈ 1 RPS

- Assumption: Peak Load Factor = 5x
- Peak RPS = 1 * 5 = 5 RPS
```

<span style="color:green;font-weight:bold">Summary</span>

| Operation          | Total Requests / Day | Avg RPS | Peak RPS (5x) |
| ------------------ | -------------------- | ------- | ------------- |
| Login User         | 1,000,000            | 10      | 50            |
| Search Hotels      | 25,000,000           | 250     | 1,250         |
| View Hotel Details | 5,000,000            | 50      | 250           |
| Book Hotel Room    | 100,000              | 1       | 5             |

---
### Storage Estimation

Designing a scalable system requires a clear understanding of how much data the platform needs to store and manage over time. **Storage estimation** helps us anticipate database size, plan for indexing, implement backup strategies, and decide whether to adopt **sharding or archiving** for future growth. 

Let's estimate the storage requirements for key entities:

<span style="color:purple;font-weight:bold">Users</span>

This entity forms the basis for identity management, personalization, and secure access across the platform.

```
- User ID: ~16 bytes (UUID or numeric ID)
- Name: ~100 bytes (First Name + Last Name)
- Email: ~100 bytes (can be long, e.g., corporate emails)
- Phone Number: ~20 bytes (include country code + separators)
- Hashed Password: ~256 bytes (depending on algorithm)
- Other Attributes: ~500 bytes (profile image URL, addresses, preferences, etc.)
```

```
- (Assumption) Total Registered Users: 50 Million
- Average User Record Size: 1 KB (with indexes & overhead)
- Total Storage = 50,000,000 * 1 KB = 50,000,000 KB = 50 GB
```

<span style="color:purple;font-weight:bold">Hotel</span>

The **Hotel entity** serves as the foundation for the booking platform, providing the details users need to discover and evaluate hotels before making a reservation. It would contain metadata such as:

```
- Hotel ID: ~16 bytes (UUID or numeric ID)
- Name: ~100 bytes (typically under 50 characters)
- Location: ~250 bytes (full address, city, state, country)  
- Description: ~2 KB (maximum 2,000 characters)
- Amenities: ~500 bytes (list of 10 amenities, e.g., “Wi-Fi, Pool”)
- Images Metadata: ~2 KB (10 image URLs, each ~200 bytes)
```

Based on these field-level estimates, we can now approximate the overall storage required for hotels:

```
- Assumption: Total Hotels = 1 Million
- Average Hotel Record Size = 5 KB (with indexes & overhead)
- Total Storage for Hotels = 1,000,000 × 5 KB = 5 GB
```

> **NOTE:** Assuming each hotel has **10 images** and each image is around **500 KB**, the total storage required for storing all images would be: `1,000,000 × 10 × 500 KB = 5,000,000,000 KB = 5 TB`.

<span style="color:purple;font-weight:bold">Room</span>

If rooms were stored as an **embedded list inside the Hotel entity**, updating a single room (e.g., changing its availability or price) would require **updating the entire hotel record**. This could lead to **write conflicts** and require **locking the whole hotel record**, reducing performance under high load. 

By keeping **Room** as a separate entity, each room can be updated independently, minimising lock contention and improving **scalability**, **performance**, and **concurrency handling**.

```
- Room ID: ~16 bytes (UUID or numeric ID)
- Room Type: ~100 bytes (e.g., "Deluxe", "Standard")  
- Price: ~50 bytes (price value + currency)  
- Availability: ~250 bytes (calendar)  
- Other Attributes: ~500 bytes (bed type, occupancy, amenties, etc.)
```

```
- Assumption: Average Rooms per Hotel = 100
- Average Room Record Size = 1 KB (with indexes & overhead)  
- Total Storage for Rooms = 1,000,000 * 100 * 1 KB = 100 GB
```

<span style="color:purple;font-weight:bold">Booking</span>

This entity represents the transactional data that links a user to a specific room in a hotel for a defined date range.

```
- User ID: ~16 bytes (UUID or numeric ID)
- Hotel ID: ~16 bytes
- Room ID: ~16 bytes
- Dates: ~32 bytes (check-in and check-out dates)
- Payment status: ~20 bytes (e.g., “Paid”, “Pending”)
- Other Attributes: ~500 bytes (created timestamp, updated timestamp, etc.)
```

Since booking records are transactional in nature, they are most frequently queried within the first **12 months** (for cancellations, modifications, customer support, disputes, etc.). After this retention window, the likelihood of accessing historical bookings drops significantly.

```
- Assumption: Daily Bookings = 100,000 (from load estimation)
- Assumption: Booking Retention = 1 year (365 days)
- Total Bookings per Year = 100,000 * 365 = 36.5 Million
- Booking Record Size = 1 KB (with indexes & overhead)
- Total Storage for Bookings = 36,500,000 * 1 KB = 36.5 GB
```

> **NOTE:** To optimize performance and reduce storage costs, move older bookings from the **OLTP (Online Transaction Processing) database** to an **OLAP (Online Analytical Processing) database**.

<span style="color:green;font-weight:bold">Summary</span>

| **Entity**       | **Estimated Size per Record** | **Total Records** | **Total Storage** |
| ---------------- | ----------------------------- | ----------------- | ----------------- |
| **Hotels**       | 5 KB                          | 1,000,000         | 5 GB              |
| **Hotel Images** | 500 KB                        | 10,000,000        | 5 TB              |
| **Rooms**        | 1 KB                          | 100,000,000       | 100 GB            |
| **Users**        | 1 KB                          | 50,000,000        | 50 GB             |
| **Bookings**     | 1 KB                          | 36,500,000        | 36.5 GB           |

---
### Bandwidth Estimation

**Bandwidth estimation** measures how much data flows into and out of the system each second. Quantifying this helps size **outbound network throughput**, justify **caching**, and decide where a **CDN** is essential.

<span style="color:red;">(in hotel-booking platform, egress throughput will be much higher than ingress throughput)</span>

Let's estimate the outbound network throughput for the core use cases:

<span style="color:purple;font-weight:bold">Search Hotels</span>

```
- Assumption: 20 Hotel Records sent per Request (pagination)
- Single Hotel Record Size ≈ 5 KB
- Payload Size = 20 * 5 KB = 100 KB

- Average RPS ≈ 250 RPS
- Average Throughput = 100 KB * 250 RPS 
					 = 25,000 KB/s
					 ≈ 25 MB/s
					 = 200 Mbps 

- Peak RPS ≈ 1,250 RPS
- Peak Throughput = 100 KB * 1,250 RPS
				  = 125,000 KB/s
				  ≈ 125 MB/s
				  = 1000 Mbps (~1 Gbps)
```

<span style="color:purple;font-weight:bold">View Details of Selected Hotel</span>

```
- Assumption: 10 images fetched sent per Request (load estimation)
- Payload Size = Single Hotel Record Size + 10 Images Size 
			   = 5 KB + 10 * 500 KB ≈ 5 MB 

- Average RPS ≈ 50 RPS
- Average Throughput = 5 MB * 50 RPS
					 = 250 MB/s
					 = 2000 Mbps (~2 Gbps)

- Peak RPS ≈ 250 RPS
- Peak Throughput = 5 MB * 250 RPS
				  = 1250 MB/s
				  = 10,000 Mbps (~10 Gbps)
```

<span style="color:green;font-weight:bold;">Summary</span>

| Operation          | Avg Throughput      | Peak Throughput        |
| ------------------ | ------------------- | ---------------------- |
| Search Hotels      | 200 Mbps            | 1000 Mbps (~1 Gbps)    |
| View Hotel Details | 2000 Mbps (~2 Gbps) | 10,000 Mbps (~10 Gbps) |

---
### System Architecture

Designing the system architecture begins with mapping the estimated load, storage, and bandwidth requirements into concrete design decisions.

The back-of-the-envelope estimations suggested **distinct traffic patterns** and **storage requirements** for each use case. A microservices architecture suits our hotel-booking platform because it allows us to **divide responsibilities into services that can be deployed and scaled independently**, ensuring each one is tailored to its specific workload.

The most important design decision in a microservices architecture is **where to place service boundaries**. For a hotel-reservation platform, the cleanest seams align with distinct domain responsibilities, data ownership, and traffic patterns:

<span style="color:purple;font-weight:bold;">Hotel Service</span>

---

<span style="color:red;">(continue from here)</span>

why hotel service should not handle search hotel and book room requests?

hotels and rooms live in a sharded relational (or NewSQL) cluster to preserve transactional semantics while scaling horizontally: we partition by a hash of `hotel_id` for even load, or by region where data locality is important

a Details API for hotel metadata, Details at three pods (headroom for spikes),

Peak traffic is dominated by search (~1,250 RPS) and details reads; assuming ~500 RPS per stateless instance at P95 < 120 ms.

<span style="color:purple;font-weight:bold;">Search Service</span>

If the **Hotel Service** alone is responsible for serving user searches (e.g., “Find hotels in New York under $200 with Wi-Fi and breakfast”), every query would require the service to scan large amounts of hotel and room data directly from its relational database. 

As the number of hotels grows (millions globally), filtering on multiple attributes—price ranges, locations, amenities, availability—quickly becomes **expensive and slow**. Traditional relational queries may need complex joins across hotels and rooms, leading to **high query latency**, **heavy load on the primary database**, and degraded performance for critical operations like reservations that share the same database. In other words, the system risks becoming both **slow** and **unreliable** under search-heavy traffic.

The **Search Service** addresses this bottleneck by maintaining a **denormalized, inverted index** (e.g., in Elasticsearch or OpenSearch) optimized for fast text search and multi-field filtering. Instead of repeatedly hitting the transactional Hotel Service database, user search queries are directed to this index, which stores pre-flattened hotel documents containing names, locations, pricing snapshots, and amenity tags. Queries like “New York + Pool + under $200” can then be executed in **milliseconds** using inverted indexes, faceted search, and geospatial filtering—operations that relational databases are not optimized for.

Additionally, the Search Service can scale independently (more nodes for higher query throughput) and absorb thousands of read requests per second without affecting the **Hotel Service’s primary OLTP database**, which remains reserved for updates, writes, and consistency-critical operations. Updates to hotel records flow into the Search Service asynchronously via **change data capture (CDC)** or event streams, keeping the index fresh without slowing down the main transactional path.

**Search** (indexed, denormalized read model over catalog)

a Search API serving the high-read path, we start the Search API at four to five pods (with HPA on CPU/QPS),

Search is served from an OpenSearch/Elasticsearch tier with regional indices and hash shards; we denormalize the document (core hotel facets + a few room attributes) to keep search fast without fan-out reads.

<span style="color:purple;font-weight:bold;">Reservation Service</span>

a Booking service for ACID transactions, Booking at three pods (throughput is low but consistency and redundancy matter)

Bookings carry both a strong consistency requirement and natural time boundaries, so we use composite partitioning by month plus a secondary hash, retain 12 months online, and stream older partitions to an OLAP warehouse for analytics.

<span style="color:purple;font-weight:bold;">Inventory Service</span>

If the **Reservation Service** is used for every availability check, each search or detail request would need to scan existing reservations to determine whether rooms are still free for a given date range. At scale, with **millions of bookings**, this becomes very inefficient:

- **High read load on a write-heavy service:** the Reservation Service is optimized for ACID writes (creating/canceling bookings), not for read-heavy queries. Mixing the two creates lock contention, slowing down both searches and reservations.
    
- **Latency and scalability issues:** checking availability by querying raw bookings requires range scans and complex logic, leading to high response times under heavy search traffic (hundreds/thousands of RPS).
    
- **Coupling and reliability risks:** every search request hitting the Reservation Service directly ties user browsing to the transactional booking path. A spike in searches could overload reservations, causing booking failures and poor reliability.

A dedicated **Inventory Service** decouples **availability checks** from **reservation writes**, providing a scalable read-optimized path for search and detail queries.

- **Pre-aggregated inventory:** Instead of scanning bookings, the Inventory Service maintains **counters or calendars**(e.g., number of available rooms per hotel/room/date). Each booking increments or decrements these counters atomically.
    
- **Fast lookups:** Queries like “Is Room X available from Jan 10–12?” become simple counter or calendar lookups, which are extremely efficient compared to scanning all reservations.
    
- **Isolation from booking traffic:** The Reservation Service only writes booking data and publishes events (e.g., `BookingConfirmed`, `BookingCancelled`). The Inventory Service subscribes to these events and updates its availability state. This isolates high-volume reads from the write-heavy reservation workflow.
    
- **Scalability:** The Inventory Service can scale independently (e.g., horizontally on Redis or a purpose-built availability store) to handle thousands of RPS without affecting the transactional reservation path.
    
- **Consistency:** With short-TTL caching (5–30s) and event-driven updates, the Inventory Service provides near real-time accuracy while protecting the Reservation DB from being hammered by read traffic.

**Availability** (inventory and date-range checks)

an Availability service for fast date-range checks, Availability at three to four pods (low latency target),

<span style="color:purple;font-weight:bold;">Pricing Service</span>

If the Hotel Service also computes prices, every search/detail request forces it to join hotel+room data with **rate plans, seasons, taxes, currency, promotions, loyalty, and inventory**. That makes reads:

- **Slow & costly**: complex joins/recomputations per request under high RPS.
    
- **Coupled & brittle**: pricing rules/experiments collide with catalog changes; a pricing bug degrades all browsing.
    
- **Inconsistent**: surge/blackout updates race with reads; no clear versioning/audit trail.
    
- **Hard to scale**: you must scale the entire Hotel Service for what is really a pricing hotspot.  
    Result: higher latency, DB contention with bookings, and limited agility for promotions or dynamic pricing.
    

### How a dedicated Pricing Service fixes it

A **Pricing Service** owns all price logic and data, returning a **final, breakdowned price** for (hotel, room, date-range, market, user segment):

- **Right data model**: rate plans, calendars, seasons, taxes/fees, currency FX, coupons, B2B contracts—versioned and auditable.
    
- **Right computation strategy**:
    
    - **Precompute/cache** per (room-night, market, segment) with short TTLs;
        
    - **Compute-on-read** for edge cases;
        
    - **Event-driven invalidation** on rule/FX/inventory changes.
        
- **Right scaling/SLA**: scale Pricing horizontally without touching Catalog; keep a tight P95 (e.g., <80–120 ms) even during promos.
    
- **Consistency & transparency**: deterministic rules engine, **price versioning**, and a clear **price breakdown** (base, taxes, fees, discounts) for UI and audits.
    
- **Operational safety**: you can canary new promos/rules, roll back fast, and isolate failures (a pricing spike won’t take down hotel metadata).

**Pricing** (rules and dynamic rates). 

<span style="color:purple;font-weight:bold;">Additional Services</span>

<span style="color:red;">(Add small summary for additional services like user, payment, notification, logging & monitoring)</span> At the edge, a managed load balancer fronts an API gateway that handles routing, auth offload, and rate-limits. and a lightweight Auth service. Async workers handle notifications and payment orchestration so the synchronous path stays tight. Redis is deployed as a cluster so hot keys can be spread by consistent hashing, and object storage holds all media while the OLTP stores only URLs and metadata. Because images dominate bandwidth, a CDN is non-negotiable. We place a global CDN (CloudFront/Fastly/Akamai) in front of object storage and serve multiple responsive variants (thumb/medium/large) with automatic format negotiation (WebP/AVIF).


<span style="color:green;font-weight:bold;">AWS Architecture Diagram</span>

```
AWS System Architecture
```

---
### EXTRA

Everything is containerized on Kubernetes across at least two AZs so horizontal scaling absorbs bursts without single-AZ hotspots.

Operating in a distributed network means designing for failure, latency, and consistency. We deploy across multiple AZs by default and consider active-active read regions where regulatory or latency constraints apply.

bookings remain strongly consistent via a single logical writer per region or a consensus-backed database.

we coordinate cross-service workflows with the saga pattern and an outbox to ensure messages are never lost. Every network hop is wrapped with timeouts, retries with jitter, and circuit breakers to prevent retry storms. 
 
Rate-limits and bulkheads on the gateway keep non-critical traffic from starving critical paths. 

---

<span style="color:purple;font-weight:bold">Login User</span>

```

```


<span style="color:purple;font-weight:bold">Book a Hotel Room</span>

```
- Payload Size = 1 KB

- Average RPS = 1 RPS
- Average Throughput = 1 KB * 1 RPS = 1 KB/s ≈ 0.008 Mbps

- Peak RPS = 5 RPS
- Peak Throughput = 1 KB * 5 RPS = 5 KB/s ≈ 0.04 Mbps
```

---

<span style="color:purple;font-weight:bold;">User Service</span>

<span style="color:purple;font-weight:bold;">Payment Service</span>

<span style="color:purple;font-weight:bold;">Notification Service</span>

<span style="color:purple;font-weight:bold;">Logging & Notification Service</span>

---


